{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обуяение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from khajiit_common.intermediate_results_caching import Vault\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1027 05:30:31.581629 140488245839680 modeling_bert.py:139] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1027 05:30:31.588151 140488245839680 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import random\n",
    "import pytorch_transformers\n",
    "import logging\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 2\"\n",
    "from path import Path\n",
    "from pytorch_transformers import BertTokenizer, BertPreTrainedModel, BertModel\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "from sklearn_pytorch.nlp import BertPreprocessor\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, TopKCategoricalAccuracy\n",
    "from ignite.contrib.handlers import ProgressBar, TensorboardLogger\n",
    "from ignite.contrib.handlers.tensorboard_logger import OutputHandler, OptimizerParamsHandler\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_TEST = ['id', 'title', 'text']\n",
    "HEADER_TRAIN = ['id', 'title', 'text', 'tags']\n",
    "RANDOM_SEED = 442\n",
    "TEST_SIZE = .1\n",
    "EXPERIMENT_NAME = 'ya_champ_multiclass_full_data_preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed, cudnn_deterministic=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if cudnn_deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_iterator(file):\n",
    "    for line in file:\n",
    "        yield line #.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, col_no=4, header=HEADER_TRAIN):\n",
    "    count = 0\n",
    "    junk_rows = {k: [] for k in np.arange(col_no)}\n",
    "    junk_row_numbers = {k: [] for k in np.arange(col_no)}\n",
    "    train_data_clean = {k: [] for k in header}\n",
    "    with open(path, encoding='utf-8') as src_file:\n",
    "        f_iter = get_file_iterator(src_file)\n",
    "        for i, line in enumerate(f_iter):\n",
    "            count += 1\n",
    "            row = line.replace('\\n', '').split('\\t')\n",
    "            if len(row) >= col_no - 1:\n",
    "                if len(row) == col_no:\n",
    "                    for j, k in enumerate(header):\n",
    "                        train_data_clean[k].append(row[j].strip())\n",
    "                elif col_no == 3:\n",
    "                    train_data_clean[header[0]].append(row[0])\n",
    "                    train_data_clean[header[1]].append(' ')\n",
    "                    train_data_clean[header[2]].append(row[-1].strip())\n",
    "#                     else:\n",
    "#                         train_data_clean[header[2]].append(row[-2])\n",
    "#                         train_data_clean[header[3]].append(row[-1])\n",
    "                    \n",
    "    #         if count > 10:\n",
    "    #             break\n",
    "            else:\n",
    "                junk_rows[len(row)].append(row)\n",
    "                junk_row_numbers[len(row)].append(i)\n",
    "    train_data_clean = pd.DataFrame.from_dict(train_data_clean)\n",
    "    train_data_clean['concatted_text'] = train_data_clean[['title', 'text']].apply(lambda row: '. '.join([row['title'], row['text']]).strip(), axis=1)\n",
    "    train_data_clean.loc[train_data_clean['concatted_text'].str.startswith('. '), 'concatted_text'] = train_data_clean.loc[train_data_clean['concatted_text'].str.startswith('. '), 'concatted_text'].str[2:]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    if col_no > 3:\n",
    "        train_data_clean['tags'] = train_data_clean['tags'].apply(lambda x: [int(nmbr) for nmbr in x.split(',')])\n",
    "        mlb.fit(train_data_clean['tags'])\n",
    "        train_data_clean['tags'] = [tag_list for tag_list in mlb.transform(train_data_clean['tags'])]\n",
    "    return train_data_clean, mlb, junk_rows, junk_row_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean, mlb, junk_rows, junk_row_numbers = get_data('data/train.tsv', 4, HEADER_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vault = Vault('processed', 'intermediate_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vault.train_clean = train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = vault.train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# we'll be writing to stdout and to log file for reliability\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%Y-%m-%d %H:%M:%S',\n",
    "                    level = logging.INFO,\n",
    "                    handlers = [logging.StreamHandler(sys.stdout),\n",
    "                                logging.FileHandler(f'logs/{EXPERIMENT_NAME}.log', encoding='utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1027 05:30:47.838646 140488245839680 <ipython-input-16-aacf97904110>:1] device: cuda n_gpu: 2\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"device: {} n_gpu: {}\".format(device, n_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать BERT, реализацию https://github.com/huggingface/transformers/. Веса возьмем у DeepPavlov. http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v2.tar.gz\n",
    "Checkpoint для tensorflow. Он был сконвертирован в pytorch скриптом: https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/convert_tf_checkpoint_to_pytorch.py , когда он еще был в репозитории.\n",
    "Ссылка на сконвертированную модель:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_dir = Path('/work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1027 05:30:48.030088 140488245839680 tokenization_utils.py:301] Model name '/work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/' is a path or url to a directory containing tokenizer files.\n",
      "I1027 05:30:48.032034 140488245839680 tokenization_utils.py:330] Didn't find file /work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/added_tokens.json. We won't load it.\n",
      "I1027 05:30:48.033525 140488245839680 tokenization_utils.py:330] Didn't find file /work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/special_tokens_map.json. We won't load it.\n",
      "I1027 05:30:48.034562 140488245839680 tokenization_utils.py:330] Didn't find file /work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/tokenizer_config.json. We won't load it.\n",
      "I1027 05:30:48.035890 140488245839680 tokenization_utils.py:365] loading file /work/work/word_embeddings/for_torch/rubert_cased_L-12_H-768_A-12_v1/dump_path/vocab.txt\n",
      "I1027 05:30:48.036878 140488245839680 tokenization_utils.py:365] loading file None\n",
      "I1027 05:30:48.037878 140488245839680 tokenization_utils.py:365] loading file None\n",
      "I1027 05:30:48.038877 140488245839680 tokenization_utils.py:365] loading file None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(pretrained_bert_dir,\n",
    "                                          do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocessor = BertPreprocessor(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем на трейн и валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(data, return_subsample=False):\n",
    "    train, val = train_test_split(data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "    if return_subsample:\n",
    "        train = train.sample(100, random_state=RANDOM_SEED).copy()\n",
    "        val = val.sample(100, random_state=RANDOM_SEED).copy()\n",
    "    y_train = np.vstack(train['tags'].values)\n",
    "    y_val = np.vstack(val['tags'].values)\n",
    "    return train, val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, y_train, y_val = get_train_val_data(train_clean, return_subsample=True)\n",
    "\n",
    "train, val, y_train, y_val = get_train_val_data(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = bert_preprocessor.transform(train['concatted_text'])\n",
    "\n",
    "val_features = bert_preprocessor.transform(val['concatted_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_dataset(features, target_column):\n",
    "    all_input_ids = torch.tensor(features['input_ids'], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor(features['token_type_ids'], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(features['attention_mask'], dtype=torch.long)\n",
    "    \n",
    "    all_label_map = torch.tensor(target_column, dtype=torch.float32)\n",
    "    \n",
    "    return TensorDataset(all_input_ids, all_token_type_ids, all_attention_mask, all_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_tensor_dataset(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = get_tensor_dataset(val_features, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самая похожая по назначению вариация БЕРТа у huggingface - BertForSequenceClassification - однаклассовая классификация текстов. Чтобы превратить ее в многоклассовую, будем использовать другой лосс - бинарную кросс-энтропию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMCSequenceClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForMCSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids, \n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            loss = loss_fct(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mlb\n",
    "except NameError:\n",
    "    mlb = None   \n",
    "\n",
    "num_labels = len(mlb.classes_) if mlb else 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMCSequenceClassification.from_pretrained(pretrained_bert_dir,\n",
    "                                                      num_labels=num_labels,\n",
    "                                                      output_attentions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "\n",
    "learning_rate = 0.00003\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "train_batch_size = 7 * n_gpu\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "test_batch_size = train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = SequentialSampler(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "num_train_optimization_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_steps = int(num_train_optimization_steps * warmup_proportion)\n",
    "num_warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_needs_decay = lambda name: False if re.search(r'bias|LayerNorm\\.weight', name) else True\n",
    "optimizer_grouped_parameters = [\n",
    "    {'weight_decay': 0.01, 'params': [p for n, p in model.named_parameters() if param_needs_decay(n)]},\n",
    "    {'weight_decay': 0.0, 'params': [p for n, p in model.named_parameters() if not param_needs_decay(n)]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=learning_rate,\n",
    "                  correct_bias=False, # to reproduce BertAdam specific behavior\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                 warmup_steps=num_warmup_steps,\n",
    "                                 t_total=num_train_optimization_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(engine, batch):\n",
    "    model.train()\n",
    "    \n",
    "    input_ids, token_type_ids, attention_mask, label_ids = (t.to(device) for t in batch)\n",
    "    \n",
    "    logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    \n",
    "    loss = criterion(logits, label_ids)\n",
    "    loss /= gradient_accumulation_steps\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if (engine.state.iteration + 1) % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    return loss.item(), logits, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids, token_type_ids, attention_mask, label_ids = (t.to(device) for t in batch)\n",
    "\n",
    "        logits = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        return logits, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(process_function)\n",
    "evaluator = Engine(eval_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda output: output[0]).attach(trainer, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss(criterion).attach(evaluator, 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True, bar_format='')\n",
    "pbar.attach(trainer, ['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(engine):\n",
    "    metrics = engine.state.metrics\n",
    "    \n",
    "    evaluator.run(val_dataloader)\n",
    "    val_metrics = evaluator.state.metrics\n",
    "    \n",
    "    engine.state.val_metrics = val_metrics\n",
    "    \n",
    "    logger.info(f'Training Results - Epoch: {engine.state.epoch:2} loss={metrics[\"loss\"]:.3f}')\n",
    "    \n",
    "    logger.info(f'Validation Results - Epoch: {engine.state.epoch:2} loss={val_metrics[\"val_loss\"]:.3f}')\n",
    "    \n",
    "    pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(f'saved_models/{EXPERIMENT_NAME}',\n",
    "                               'bert_classifier',\n",
    "                               score_function=lambda engine: -engine.state.val_metrics['val_loss'],\n",
    "                               score_name='val_loss',\n",
    "                               n_saved=2,\n",
    "                               create_dir=True,\n",
    "                               save_as_state_dict=True,\n",
    "                               require_empty=True)\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'model_state_dict': model.module if hasattr(model, 'module') else model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = TensorboardLogger(f'runs/{EXPERIMENT_NAME}')\n",
    "\n",
    "# Attach the logger to the trainer to log training loss at each iteration\n",
    "tb_logger.attach(trainer,\n",
    "                 log_handler=OutputHandler(tag=\"training\", output_transform=lambda output: {'loss': output[0]}),\n",
    "                 event_name=Events.ITERATION_COMPLETED)\n",
    "\n",
    "# Attach the logger to the trainer to log optimizer's parameters, e.g. learning rate at each iteration\n",
    "tb_logger.attach(trainer,\n",
    "                 log_handler=OptimizerParamsHandler(optimizer),\n",
    "                 event_name=Events.ITERATION_STARTED)\n",
    "\n",
    "# Attach the logger to the evaluator and metrics after each epoch\n",
    "# We setup `another_engine=trainer` to take the epoch of the `trainer` instead of `evaluator`.\n",
    "tb_logger.attach(evaluator,\n",
    "                 log_handler=OutputHandler(tag=\"validation\",\n",
    "                                           metric_names=[\"val_loss\"],\n",
    "                                           another_engine=trainer),\n",
    "                 event_name=Events.EPOCH_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vault.train_data_sigmoid = train_dataset\n",
    "# vault.val_data_sigmoid = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_dataloader, max_epochs=num_train_epochs)\n",
    "\n",
    "tb_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
